{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOGL stock 'Close' value prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock data function configured to drop all columns except 'Open','High' and 'Close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name, normalized=0):\n",
    "    url=\"http://www.google.com/finance/historical?q=\"+stock_name+\"&startdate=Jul+12%2C+2013&enddate=Jul+11%2C+2017&num=30&ei=rCtlWZGSFN3KsQHwrqWQCw&output=csv\"\n",
    "#     url=\"http://www.google.com/finance/historical?q=%s&ei=u-lHWfGPNNWIsgHHqIqICw&output=csv\" % stock_name \n",
    "\n",
    "\n",
    "    col_names = ['Date','Open','High','Low','Close','Volume']\n",
    "    stocks = pd.read_csv(url, header=0, names=col_names) \n",
    "    df = pd.DataFrame(stocks)\n",
    "    df.drop(df.columns[[0,3,5]], axis=1, inplace=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GOOGL stock data from yahoo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>459.95</td>\n",
       "      <td>460.46</td>\n",
       "      <td>455.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>460.96</td>\n",
       "      <td>463.89</td>\n",
       "      <td>459.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>464.03</td>\n",
       "      <td>464.45</td>\n",
       "      <td>460.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>462.61</td>\n",
       "      <td>464.46</td>\n",
       "      <td>462.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>460.46</td>\n",
       "      <td>461.96</td>\n",
       "      <td>461.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open    High   Close\n",
       "1002  459.95  460.46  455.80\n",
       "1003  460.96  463.89  459.73\n",
       "1004  464.03  464.45  460.26\n",
       "1005  462.61  464.46  462.81\n",
       "1006  460.46  461.96  461.96"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_name = 'GOOGL'\n",
    "df = get_stock_data(stock_name,0)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data to a file for a future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "file_name = stock_name+'_stock_%s.csv' % today\n",
    "df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.94195</td>\n",
       "      <td>0.95313</td>\n",
       "      <td>0.95100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.93098</td>\n",
       "      <td>0.94466</td>\n",
       "      <td>0.94081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.92500</td>\n",
       "      <td>0.93614</td>\n",
       "      <td>0.92769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.92420</td>\n",
       "      <td>0.93629</td>\n",
       "      <td>0.93226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.93322</td>\n",
       "      <td>0.93424</td>\n",
       "      <td>0.91946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open     High    Close\n",
       "0  0.94195  0.95313  0.95100\n",
       "1  0.93098  0.94466  0.94081\n",
       "2  0.92500  0.93614  0.92769\n",
       "3  0.92420  0.93629  0.93226\n",
       "4  0.93322  0.93424  0.91946"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['High'] = df['High'] / 1000\n",
    "df['Open'] = df['Open'] / 1000\n",
    "df['Close'] = df['Close'] / 1000\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated load_data function from lstm.py, configured to accept any amount of features.\n",
    "## It is set to calculate the last feature as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.as_matrix() #pd.DataFrame(stock)\n",
    "    sequence_length = seq_len + 1\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    \n",
    "  \n",
    "    \n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0])\n",
    "    train = result[:int(row), :]\n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1][:,-1]\n",
    "    x_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1]\n",
    "    \n",
    "    x_train = preprocessing.scale(x_train)\n",
    "    y_train = preprocessing.scale(y_train)\n",
    "    \n",
    "    x_test = preprocessing.scale(x_test)\n",
    "    y_test = preprocessing.scale(y_test)\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "  \n",
    "    \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        input_dim=layers[0],\n",
    "        output_dim=layers[1],\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        layers[2],\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "        output_dim=layers[2]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\",metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "def build_model2(layers):\n",
    "        d = 0.2\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(Dense(16,init='uniform',activation='relu'))        \n",
    "        model.add(Dense(1,init='uniform',activation='relu'))\n",
    "        model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting X and Y for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. the scale function expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2eb3222866ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-55e267ad62a4>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(stock, seq_len)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mscale\u001b[0;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[1;32m    127\u001b[0m     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\n\u001b[1;32m    128\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'the scale function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 405\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. the scale function expected <= 2."
     ]
    }
   ],
   "source": [
    "window = 5\n",
    "X_train, y_train, X_test, y_test = load_data(df[::-1], window)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model sequence structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, kernel_initializer=\"uniform\", activation=\"relu\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:32: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\", activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "# model = build_model([3,lag,1])\n",
    "model = build_model2([3,window,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the model & RMS/RMSE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 810 samples, validate on 91 samples\n",
      "Epoch 1/500\n",
      "810/810 [==============================] - 0s - loss: 60484.9019 - acc: 0.0000e+00 - val_loss: 173338.9844 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "810/810 [==============================] - 0s - loss: 60094.2771 - acc: 0.0000e+00 - val_loss: 171739.7188 - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "810/810 [==============================] - 0s - loss: 59692.0542 - acc: 0.0000e+00 - val_loss: 170154.3438 - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "810/810 [==============================] - 0s - loss: 58316.2473 - acc: 0.0000e+00 - val_loss: 168581.0312 - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "810/810 [==============================] - 0s - loss: 58043.6122 - acc: 0.0000e+00 - val_loss: 167021.1875 - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "810/810 [==============================] - 0s - loss: 57133.0765 - acc: 0.0000e+00 - val_loss: 165473.9844 - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "810/810 [==============================] - 0s - loss: 55887.2468 - acc: 0.0000e+00 - val_loss: 163938.1250 - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "810/810 [==============================] - 0s - loss: 55427.0538 - acc: 0.0000e+00 - val_loss: 162414.9062 - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "810/810 [==============================] - 0s - loss: 54084.3438 - acc: 0.0000e+00 - val_loss: 160906.9062 - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "810/810 [==============================] - 0s - loss: 53049.6893 - acc: 0.0000e+00 - val_loss: 159408.3281 - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "810/810 [==============================] - 0s - loss: 52705.7890 - acc: 0.0000e+00 - val_loss: 157924.2344 - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "810/810 [==============================] - 0s - loss: 52506.4368 - acc: 0.0000e+00 - val_loss: 156452.0156 - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "810/810 [==============================] - 0s - loss: 50999.0739 - acc: 0.0000e+00 - val_loss: 154992.9844 - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "810/810 [==============================] - 0s - loss: 50286.5537 - acc: 0.0000e+00 - val_loss: 153546.5625 - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "810/810 [==============================] - 0s - loss: 50430.0976 - acc: 0.0000e+00 - val_loss: 152111.9531 - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "810/810 [==============================] - 0s - loss: 48999.5238 - acc: 0.0000e+00 - val_loss: 150689.9375 - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "810/810 [==============================] - 0s - loss: 48266.1875 - acc: 0.0000e+00 - val_loss: 149282.5781 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "810/810 [==============================] - 0s - loss: 48390.7858 - acc: 0.0000e+00 - val_loss: 147890.6406 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "810/810 [==============================] - 0s - loss: 47554.9872 - acc: 0.0000e+00 - val_loss: 146511.1719 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "810/810 [==============================] - 0s - loss: 46224.1839 - acc: 0.0000e+00 - val_loss: 145143.8438 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "810/810 [==============================] - 0s - loss: 45635.6438 - acc: 0.0000e+00 - val_loss: 143790.4375 - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "810/810 [==============================] - 0s - loss: 45825.4908 - acc: 0.0000e+00 - val_loss: 142446.6094 - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "810/810 [==============================] - 0s - loss: 44650.4211 - acc: 0.0000e+00 - val_loss: 141115.0938 - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "810/810 [==============================] - 0s - loss: 43559.6930 - acc: 0.0000e+00 - val_loss: 139795.2344 - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "810/810 [==============================] - 0s - loss: 43460.7756 - acc: 0.0000e+00 - val_loss: 138486.7656 - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "810/810 [==============================] - 0s - loss: 42515.1165 - acc: 0.0000e+00 - val_loss: 137192.8125 - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "810/810 [==============================] - 0s - loss: 42123.0210 - acc: 0.0000e+00 - val_loss: 135908.7344 - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "810/810 [==============================] - 0s - loss: 41276.7227 - acc: 0.0000e+00 - val_loss: 134636.8750 - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "810/810 [==============================] - 0s - loss: 40827.9421 - acc: 0.0000e+00 - val_loss: 133378.8438 - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "810/810 [==============================] - 0s - loss: 40773.9012 - acc: 0.0000e+00 - val_loss: 132134.8906 - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "810/810 [==============================] - 0s - loss: 39775.3129 - acc: 0.0000e+00 - val_loss: 130902.7891 - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "810/810 [==============================] - 0s - loss: 38780.1207 - acc: 0.0000e+00 - val_loss: 129685.0312 - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "810/810 [==============================] - 0s - loss: 39148.4390 - acc: 0.0000e+00 - val_loss: 128475.5938 - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "810/810 [==============================] - 0s - loss: 38470.7799 - acc: 0.0000e+00 - val_loss: 127278.3516 - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "810/810 [==============================] - 0s - loss: 37881.4147 - acc: 0.0000e+00 - val_loss: 126093.0859 - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "810/810 [==============================] - 0s - loss: 36798.2547 - acc: 0.0000e+00 - val_loss: 124919.4062 - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "810/810 [==============================] - 0s - loss: 36348.1531 - acc: 0.0000e+00 - val_loss: 123759.5469 - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "810/810 [==============================] - 0s - loss: 35181.2416 - acc: 0.0000e+00 - val_loss: 122612.5391 - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "810/810 [==============================] - 0s - loss: 35022.6425 - acc: 0.0000e+00 - val_loss: 121476.8125 - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "810/810 [==============================] - 0s - loss: 35096.4556 - acc: 0.0000e+00 - val_loss: 120353.4141 - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "810/810 [==============================] - 0s - loss: 34257.7048 - acc: 0.0000e+00 - val_loss: 119240.9688 - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "810/810 [==============================] - 0s - loss: 33769.7002 - acc: 0.0000e+00 - val_loss: 118141.8047 - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "810/810 [==============================] - 0s - loss: 33666.3010 - acc: 0.0000e+00 - val_loss: 117054.1406 - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "810/810 [==============================] - 0s - loss: 33235.9764 - acc: 0.0000e+00 - val_loss: 115978.8125 - val_acc: 0.0000e+00\n",
      "Epoch 45/500\n",
      "810/810 [==============================] - 0s - loss: 32215.2921 - acc: 0.0000e+00 - val_loss: 114917.5703 - val_acc: 0.0000e+00\n",
      "Epoch 46/500\n",
      "810/810 [==============================] - 0s - loss: 32185.1826 - acc: 0.0000e+00 - val_loss: 113865.0000 - val_acc: 0.0000e+00\n",
      "Epoch 47/500\n",
      "810/810 [==============================] - 0s - loss: 31461.5612 - acc: 0.0000e+00 - val_loss: 112821.6719 - val_acc: 0.0000e+00\n",
      "Epoch 48/500\n",
      "810/810 [==============================] - 0s - loss: 31025.6108 - acc: 0.0000e+00 - val_loss: 111794.4297 - val_acc: 0.0000e+00\n",
      "Epoch 49/500\n",
      "810/810 [==============================] - 0s - loss: 30903.5417 - acc: 0.0000e+00 - val_loss: 110781.5625 - val_acc: 0.0000e+00\n",
      "Epoch 50/500\n",
      "810/810 [==============================] - 0s - loss: 29611.0577 - acc: 0.0000e+00 - val_loss: 109778.3594 - val_acc: 0.0000e+00\n",
      "Epoch 51/500\n",
      "810/810 [==============================] - 0s - loss: 30657.8031 - acc: 0.0000e+00 - val_loss: 108786.6719 - val_acc: 0.0000e+00\n",
      "Epoch 52/500\n",
      "810/810 [==============================] - 0s - loss: 29948.0116 - acc: 0.0000e+00 - val_loss: 107806.6719 - val_acc: 0.0000e+00\n",
      "Epoch 53/500\n",
      "810/810 [==============================] - 0s - loss: 29504.8150 - acc: 0.0000e+00 - val_loss: 106837.9688 - val_acc: 0.0000e+00\n",
      "Epoch 54/500\n",
      "810/810 [==============================] - 0s - loss: 28815.3577 - acc: 0.0000e+00 - val_loss: 105879.8438 - val_acc: 0.0000e+00\n",
      "Epoch 55/500\n",
      "810/810 [==============================] - 0s - loss: 28993.0199 - acc: 0.0000e+00 - val_loss: 104928.3438 - val_acc: 0.0000e+00\n",
      "Epoch 56/500\n",
      "810/810 [==============================] - 0s - loss: 28316.6666 - acc: 0.0000e+00 - val_loss: 103989.0469 - val_acc: 0.0000e+00\n",
      "Epoch 57/500\n",
      "810/810 [==============================] - 0s - loss: 27719.0964 - acc: 0.0000e+00 - val_loss: 103058.4922 - val_acc: 0.0000e+00\n",
      "Epoch 58/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810/810 [==============================] - 0s - loss: 26912.7053 - acc: 0.0000e+00 - val_loss: 102136.3047 - val_acc: 0.0000e+00\n",
      "Epoch 59/500\n",
      "810/810 [==============================] - 0s - loss: 26857.3792 - acc: 0.0000e+00 - val_loss: 101226.2344 - val_acc: 0.0000e+00\n",
      "Epoch 60/500\n",
      "810/810 [==============================] - 0s - loss: 26777.8745 - acc: 0.0000e+00 - val_loss: 100326.1172 - val_acc: 0.0000e+00\n",
      "Epoch 61/500\n",
      "810/810 [==============================] - 0s - loss: 26669.8504 - acc: 0.0000e+00 - val_loss: 99436.6797 - val_acc: 0.0000e+00\n",
      "Epoch 62/500\n",
      "810/810 [==============================] - 0s - loss: 26327.6205 - acc: 0.0000e+00 - val_loss: 98559.4844 - val_acc: 0.0000e+00\n",
      "Epoch 63/500\n",
      "810/810 [==============================] - 0s - loss: 25853.6717 - acc: 0.0000e+00 - val_loss: 97691.6016 - val_acc: 0.0000e+00\n",
      "Epoch 64/500\n",
      "810/810 [==============================] - 0s - loss: 25645.4537 - acc: 0.0000e+00 - val_loss: 96833.5859 - val_acc: 0.0000e+00\n",
      "Epoch 65/500\n",
      "810/810 [==============================] - 0s - loss: 25780.1701 - acc: 0.0000e+00 - val_loss: 95983.3516 - val_acc: 0.0000e+00\n",
      "Epoch 66/500\n",
      "810/810 [==============================] - 0s - loss: 24970.2610 - acc: 0.0000e+00 - val_loss: 95145.1875 - val_acc: 0.0000e+00\n",
      "Epoch 67/500\n",
      "810/810 [==============================] - 0s - loss: 24250.3044 - acc: 0.0000e+00 - val_loss: 94318.2500 - val_acc: 0.0000e+00\n",
      "Epoch 68/500\n",
      "810/810 [==============================] - 0s - loss: 23858.1314 - acc: 0.0000e+00 - val_loss: 93503.1797 - val_acc: 0.0000e+00\n",
      "Epoch 69/500\n",
      "810/810 [==============================] - 0s - loss: 23561.0316 - acc: 0.0000e+00 - val_loss: 92699.3438 - val_acc: 0.0000e+00\n",
      "Epoch 70/500\n",
      "810/810 [==============================] - 0s - loss: 23436.4216 - acc: 0.0000e+00 - val_loss: 91905.2969 - val_acc: 0.0000e+00\n",
      "Epoch 71/500\n",
      "810/810 [==============================] - 0s - loss: 23348.3127 - acc: 0.0000e+00 - val_loss: 91118.0078 - val_acc: 0.0000e+00\n",
      "Epoch 72/500\n",
      "810/810 [==============================] - 0s - loss: 23662.8771 - acc: 0.0000e+00 - val_loss: 90337.2109 - val_acc: 0.0000e+00\n",
      "Epoch 73/500\n",
      "810/810 [==============================] - 0s - loss: 23135.7944 - acc: 0.0000e+00 - val_loss: 89566.5078 - val_acc: 0.0000e+00\n",
      "Epoch 74/500\n",
      "810/810 [==============================] - 0s - loss: 22500.7941 - acc: 0.0000e+00 - val_loss: 88806.4375 - val_acc: 0.0000e+00\n",
      "Epoch 75/500\n",
      "810/810 [==============================] - 0s - loss: 21968.4142 - acc: 0.0000e+00 - val_loss: 88055.7344 - val_acc: 0.0000e+00\n",
      "Epoch 76/500\n",
      "810/810 [==============================] - 0s - loss: 21627.3244 - acc: 0.0000e+00 - val_loss: 87312.9844 - val_acc: 0.0000e+00\n",
      "Epoch 77/500\n",
      "810/810 [==============================] - 0s - loss: 22262.3404 - acc: 0.0000e+00 - val_loss: 86582.6406 - val_acc: 0.0000e+00\n",
      "Epoch 78/500\n",
      "810/810 [==============================] - 0s - loss: 21753.0672 - acc: 0.0000e+00 - val_loss: 85859.6250 - val_acc: 0.0000e+00\n",
      "Epoch 79/500\n",
      "810/810 [==============================] - 0s - loss: 21836.5101 - acc: 0.0000e+00 - val_loss: 85146.2344 - val_acc: 0.0000e+00\n",
      "Epoch 80/500\n",
      "810/810 [==============================] - 0s - loss: 20770.9779 - acc: 0.0000e+00 - val_loss: 84440.4062 - val_acc: 0.0000e+00\n",
      "Epoch 81/500\n",
      "810/810 [==============================] - 0s - loss: 20510.8762 - acc: 0.0000e+00 - val_loss: 83746.3984 - val_acc: 0.0000e+00\n",
      "Epoch 82/500\n",
      "810/810 [==============================] - 0s - loss: 20246.5833 - acc: 0.0000e+00 - val_loss: 83061.3359 - val_acc: 0.0000e+00\n",
      "Epoch 83/500\n",
      "810/810 [==============================] - 0s - loss: 20813.7757 - acc: 0.0000e+00 - val_loss: 82383.8203 - val_acc: 0.0000e+00\n",
      "Epoch 84/500\n",
      "810/810 [==============================] - 0s - loss: 19858.4045 - acc: 0.0000e+00 - val_loss: 81716.8672 - val_acc: 0.0000e+00\n",
      "Epoch 85/500\n",
      "810/810 [==============================] - 0s - loss: 19868.1200 - acc: 0.0000e+00 - val_loss: 81059.2500 - val_acc: 0.0000e+00\n",
      "Epoch 86/500\n",
      "810/810 [==============================] - 0s - loss: 19933.9571 - acc: 0.0000e+00 - val_loss: 80408.4766 - val_acc: 0.0000e+00\n",
      "Epoch 87/500\n",
      "810/810 [==============================] - 0s - loss: 19128.4534 - acc: 0.0000e+00 - val_loss: 79767.2656 - val_acc: 0.0000e+00\n",
      "Epoch 88/500\n",
      "810/810 [==============================] - 0s - loss: 19221.4797 - acc: 0.0000e+00 - val_loss: 79135.3594 - val_acc: 0.0000e+00\n",
      "Epoch 89/500\n",
      "810/810 [==============================] - 0s - loss: 19419.3064 - acc: 0.0012 - val_loss: 78514.7344 - val_acc: 0.0000e+00\n",
      "Epoch 90/500\n",
      "810/810 [==============================] - 0s - loss: 19621.4707 - acc: 0.0000e+00 - val_loss: 77899.3438 - val_acc: 0.0000e+00\n",
      "Epoch 91/500\n",
      "810/810 [==============================] - 0s - loss: 19502.3430 - acc: 0.0000e+00 - val_loss: 77292.9141 - val_acc: 0.0000e+00\n",
      "Epoch 92/500\n",
      "810/810 [==============================] - 0s - loss: 18967.2065 - acc: 0.0000e+00 - val_loss: 76691.7031 - val_acc: 0.0000e+00\n",
      "Epoch 93/500\n",
      "810/810 [==============================] - 0s - loss: 18189.2098 - acc: 0.0000e+00 - val_loss: 76097.9531 - val_acc: 0.0000e+00\n",
      "Epoch 94/500\n",
      "810/810 [==============================] - 0s - loss: 17953.9733 - acc: 0.0000e+00 - val_loss: 75513.6172 - val_acc: 0.0000e+00\n",
      "Epoch 95/500\n",
      "810/810 [==============================] - 0s - loss: 17821.0372 - acc: 0.0000e+00 - val_loss: 74938.0078 - val_acc: 0.0000e+00\n",
      "Epoch 96/500\n",
      "810/810 [==============================] - 0s - loss: 17853.8173 - acc: 0.0000e+00 - val_loss: 74370.1562 - val_acc: 0.0000e+00\n",
      "Epoch 97/500\n",
      "810/810 [==============================] - 0s - loss: 17514.1049 - acc: 0.0000e+00 - val_loss: 73810.5625 - val_acc: 0.0000e+00\n",
      "Epoch 98/500\n",
      "810/810 [==============================] - 0s - loss: 17979.5990 - acc: 0.0000e+00 - val_loss: 73258.2969 - val_acc: 0.0000e+00\n",
      "Epoch 99/500\n",
      "810/810 [==============================] - 0s - loss: 17232.0805 - acc: 0.0012 - val_loss: 72714.1953 - val_acc: 0.0000e+00\n",
      "Epoch 100/500\n",
      "810/810 [==============================] - 0s - loss: 17545.1710 - acc: 0.0000e+00 - val_loss: 72178.7500 - val_acc: 0.0000e+00\n",
      "Epoch 101/500\n",
      "810/810 [==============================] - 0s - loss: 17418.8938 - acc: 0.0000e+00 - val_loss: 71647.5312 - val_acc: 0.0000e+00\n",
      "Epoch 102/500\n",
      "810/810 [==============================] - 0s - loss: 17271.9898 - acc: 0.0000e+00 - val_loss: 71123.4453 - val_acc: 0.0000e+00\n",
      "Epoch 103/500\n",
      "810/810 [==============================] - 0s - loss: 17124.6368 - acc: 0.0000e+00 - val_loss: 70610.5000 - val_acc: 0.0000e+00\n",
      "Epoch 104/500\n",
      "810/810 [==============================] - 0s - loss: 16652.8228 - acc: 0.0012 - val_loss: 70105.5234 - val_acc: 0.0000e+00\n",
      "Epoch 105/500\n",
      "810/810 [==============================] - 0s - loss: 16421.0770 - acc: 0.0000e+00 - val_loss: 69609.8594 - val_acc: 0.0000e+00\n",
      "Epoch 106/500\n",
      "810/810 [==============================] - 0s - loss: 16906.1936 - acc: 0.0000e+00 - val_loss: 69119.8984 - val_acc: 0.0000e+00\n",
      "Epoch 107/500\n",
      "810/810 [==============================] - 0s - loss: 16388.0852 - acc: 0.0012 - val_loss: 68635.6094 - val_acc: 0.0000e+00\n",
      "Epoch 108/500\n",
      "810/810 [==============================] - 0s - loss: 16818.6907 - acc: 0.0000e+00 - val_loss: 68156.5625 - val_acc: 0.0000e+00\n",
      "Epoch 109/500\n",
      "810/810 [==============================] - 0s - loss: 15906.9550 - acc: 0.0000e+00 - val_loss: 67683.6797 - val_acc: 0.0000e+00\n",
      "Epoch 110/500\n",
      "810/810 [==============================] - 0s - loss: 16167.1118 - acc: 0.0000e+00 - val_loss: 67216.2969 - val_acc: 0.0000e+00\n",
      "Epoch 111/500\n",
      "810/810 [==============================] - 0s - loss: 16327.6858 - acc: 0.0000e+00 - val_loss: 66755.3438 - val_acc: 0.0000e+00\n",
      "Epoch 112/500\n",
      "810/810 [==============================] - 0s - loss: 15750.1427 - acc: 0.0000e+00 - val_loss: 66302.5469 - val_acc: 0.0000e+00\n",
      "Epoch 113/500\n",
      "810/810 [==============================] - 0s - loss: 16090.2777 - acc: 0.0000e+00 - val_loss: 65854.7500 - val_acc: 0.0000e+00\n",
      "Epoch 114/500\n",
      "810/810 [==============================] - 0s - loss: 14945.4140 - acc: 0.0000e+00 - val_loss: 65414.7695 - val_acc: 0.0000e+00\n",
      "Epoch 115/500\n",
      "810/810 [==============================] - 0s - loss: 16131.8382 - acc: 0.0000e+00 - val_loss: 64986.8945 - val_acc: 0.0000e+00\n",
      "Epoch 116/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810/810 [==============================] - 1s - loss: 15449.6735 - acc: 0.0000e+00 - val_loss: 64565.9219 - val_acc: 0.0000e+00\n",
      "Epoch 117/500\n",
      "810/810 [==============================] - 0s - loss: 15754.0949 - acc: 0.0000e+00 - val_loss: 64150.2422 - val_acc: 0.0000e+00\n",
      "Epoch 118/500\n",
      "810/810 [==============================] - 0s - loss: 15158.7753 - acc: 0.0000e+00 - val_loss: 63739.6875 - val_acc: 0.0000e+00\n",
      "Epoch 119/500\n",
      "810/810 [==============================] - 0s - loss: 15299.8876 - acc: 0.0000e+00 - val_loss: 63332.3359 - val_acc: 0.0000e+00\n",
      "Epoch 120/500\n",
      "810/810 [==============================] - 0s - loss: 14976.9788 - acc: 0.0000e+00 - val_loss: 62930.5898 - val_acc: 0.0000e+00\n",
      "Epoch 121/500\n",
      "810/810 [==============================] - 0s - loss: 15257.2636 - acc: 0.0000e+00 - val_loss: 62535.3398 - val_acc: 0.0000e+00\n",
      "Epoch 122/500\n",
      "810/810 [==============================] - 0s - loss: 14347.8045 - acc: 0.0000e+00 - val_loss: 62145.8281 - val_acc: 0.0000e+00\n",
      "Epoch 123/500\n",
      "810/810 [==============================] - 0s - loss: 14712.0220 - acc: 0.0000e+00 - val_loss: 61759.9609 - val_acc: 0.0000e+00\n",
      "Epoch 124/500\n",
      "810/810 [==============================] - 0s - loss: 14710.8532 - acc: 0.0000e+00 - val_loss: 61382.0547 - val_acc: 0.0000e+00\n",
      "Epoch 125/500\n",
      "810/810 [==============================] - 0s - loss: 15032.8271 - acc: 0.0000e+00 - val_loss: 61009.1641 - val_acc: 0.0000e+00\n",
      "Epoch 126/500\n",
      "810/810 [==============================] - 0s - loss: 14611.3168 - acc: 0.0000e+00 - val_loss: 60642.4062 - val_acc: 0.0000e+00\n",
      "Epoch 127/500\n",
      "810/810 [==============================] - 0s - loss: 14916.4389 - acc: 0.0000e+00 - val_loss: 60280.0273 - val_acc: 0.0000e+00\n",
      "Epoch 128/500\n",
      "810/810 [==============================] - 0s - loss: 14939.3907 - acc: 0.0000e+00 - val_loss: 59922.3125 - val_acc: 0.0000e+00\n",
      "Epoch 129/500\n",
      "810/810 [==============================] - 0s - loss: 14514.6848 - acc: 0.0000e+00 - val_loss: 59569.6875 - val_acc: 0.0000e+00\n",
      "Epoch 130/500\n",
      "810/810 [==============================] - 0s - loss: 14143.4419 - acc: 0.0012 - val_loss: 59223.8008 - val_acc: 0.0000e+00\n",
      "Epoch 131/500\n",
      "810/810 [==============================] - 0s - loss: 14127.7833 - acc: 0.0012 - val_loss: 58885.5820 - val_acc: 0.0000e+00\n",
      "Epoch 132/500\n",
      "810/810 [==============================] - 0s - loss: 13704.5465 - acc: 0.0000e+00 - val_loss: 58550.4297 - val_acc: 0.0000e+00\n",
      "Epoch 133/500\n",
      "810/810 [==============================] - 0s - loss: 14628.5755 - acc: 0.0000e+00 - val_loss: 58219.3750 - val_acc: 0.0000e+00\n",
      "Epoch 134/500\n",
      "810/810 [==============================] - 0s - loss: 13705.4397 - acc: 0.0000e+00 - val_loss: 57893.4453 - val_acc: 0.0000e+00\n",
      "Epoch 135/500\n",
      "810/810 [==============================] - 0s - loss: 14001.2024 - acc: 0.0000e+00 - val_loss: 57573.5508 - val_acc: 0.0000e+00\n",
      "Epoch 136/500\n",
      "810/810 [==============================] - 0s - loss: 14336.0917 - acc: 0.0000e+00 - val_loss: 57258.6797 - val_acc: 0.0000e+00\n",
      "Epoch 137/500\n",
      "810/810 [==============================] - 0s - loss: 14460.8982 - acc: 0.0000e+00 - val_loss: 56948.5547 - val_acc: 0.0000e+00\n",
      "Epoch 138/500\n",
      "810/810 [==============================] - 0s - loss: 14005.4944 - acc: 0.0000e+00 - val_loss: 56643.5430 - val_acc: 0.0000e+00\n",
      "Epoch 139/500\n",
      "810/810 [==============================] - 0s - loss: 14304.6428 - acc: 0.0000e+00 - val_loss: 56343.3359 - val_acc: 0.0000e+00\n",
      "Epoch 140/500\n",
      "810/810 [==============================] - 0s - loss: 13608.3705 - acc: 0.0000e+00 - val_loss: 56046.5234 - val_acc: 0.0000e+00\n",
      "Epoch 141/500\n",
      "810/810 [==============================] - 0s - loss: 13572.0921 - acc: 0.0012 - val_loss: 55754.0938 - val_acc: 0.0000e+00\n",
      "Epoch 142/500\n",
      "810/810 [==============================] - 0s - loss: 13460.9876 - acc: 0.0000e+00 - val_loss: 55463.2852 - val_acc: 0.0000e+00\n",
      "Epoch 143/500\n",
      "810/810 [==============================] - 0s - loss: 13728.8452 - acc: 0.0000e+00 - val_loss: 55176.1328 - val_acc: 0.0000e+00\n",
      "Epoch 144/500\n",
      "810/810 [==============================] - 0s - loss: 13439.1843 - acc: 0.0000e+00 - val_loss: 54894.6367 - val_acc: 0.0000e+00\n",
      "Epoch 145/500\n",
      "810/810 [==============================] - 0s - loss: 13466.3382 - acc: 0.0000e+00 - val_loss: 54619.3633 - val_acc: 0.0000e+00\n",
      "Epoch 146/500\n",
      "810/810 [==============================] - 0s - loss: 13710.3313 - acc: 0.0000e+00 - val_loss: 54345.9609 - val_acc: 0.0000e+00\n",
      "Epoch 147/500\n",
      "810/810 [==============================] - 0s - loss: 13268.0465 - acc: 0.0000e+00 - val_loss: 54078.5000 - val_acc: 0.0000e+00\n",
      "Epoch 148/500\n",
      "810/810 [==============================] - 0s - loss: 14108.1697 - acc: 0.0000e+00 - val_loss: 53815.7148 - val_acc: 0.0000e+00\n",
      "Epoch 149/500\n",
      "810/810 [==============================] - 0s - loss: 13666.7439 - acc: 0.0000e+00 - val_loss: 53560.9883 - val_acc: 0.0000e+00\n",
      "Epoch 150/500\n",
      "810/810 [==============================] - 0s - loss: 13022.3139 - acc: 0.0000e+00 - val_loss: 53308.5391 - val_acc: 0.0000e+00\n",
      "Epoch 151/500\n",
      "810/810 [==============================] - 0s - loss: 13321.7953 - acc: 0.0000e+00 - val_loss: 53063.5508 - val_acc: 0.0000e+00\n",
      "Epoch 152/500\n",
      "810/810 [==============================] - 0s - loss: 13293.0750 - acc: 0.0000e+00 - val_loss: 52821.1641 - val_acc: 0.0000e+00\n",
      "Epoch 153/500\n",
      "810/810 [==============================] - 0s - loss: 13601.6236 - acc: 0.0000e+00 - val_loss: 52584.0508 - val_acc: 0.0000e+00\n",
      "Epoch 154/500\n",
      "810/810 [==============================] - 0s - loss: 12950.9437 - acc: 0.0000e+00 - val_loss: 52350.2188 - val_acc: 0.0000e+00\n",
      "Epoch 155/500\n",
      "810/810 [==============================] - 0s - loss: 13337.2941 - acc: 0.0000e+00 - val_loss: 52120.5312 - val_acc: 0.0000e+00\n",
      "Epoch 156/500\n",
      "810/810 [==============================] - 0s - loss: 13077.3230 - acc: 0.0000e+00 - val_loss: 51891.4609 - val_acc: 0.0000e+00\n",
      "Epoch 157/500\n",
      "810/810 [==============================] - 0s - loss: 13478.3484 - acc: 0.0000e+00 - val_loss: 51666.2930 - val_acc: 0.0000e+00\n",
      "Epoch 158/500\n",
      "810/810 [==============================] - 0s - loss: 13282.7997 - acc: 0.0000e+00 - val_loss: 51443.6992 - val_acc: 0.0000e+00\n",
      "Epoch 159/500\n",
      "810/810 [==============================] - 1s - loss: 12998.9135 - acc: 0.0000e+00 - val_loss: 51225.6484 - val_acc: 0.0000e+00\n",
      "Epoch 160/500\n",
      "810/810 [==============================] - 1s - loss: 12983.5001 - acc: 0.0000e+00 - val_loss: 51011.6602 - val_acc: 0.0000e+00\n",
      "Epoch 161/500\n",
      "810/810 [==============================] - 0s - loss: 12558.8504 - acc: 0.0000e+00 - val_loss: 50802.2578 - val_acc: 0.0000e+00\n",
      "Epoch 162/500\n",
      "810/810 [==============================] - 0s - loss: 13006.6048 - acc: 0.0000e+00 - val_loss: 50594.7188 - val_acc: 0.0000e+00\n",
      "Epoch 163/500\n",
      "810/810 [==============================] - 0s - loss: 12904.2283 - acc: 0.0000e+00 - val_loss: 50389.1055 - val_acc: 0.0000e+00\n",
      "Epoch 164/500\n",
      "810/810 [==============================] - 0s - loss: 12753.3742 - acc: 0.0000e+00 - val_loss: 50185.5977 - val_acc: 0.0000e+00\n",
      "Epoch 165/500\n",
      "810/810 [==============================] - 0s - loss: 13137.3686 - acc: 0.0000e+00 - val_loss: 49983.3242 - val_acc: 0.0000e+00\n",
      "Epoch 166/500\n",
      "810/810 [==============================] - 0s - loss: 13047.3320 - acc: 0.0000e+00 - val_loss: 49783.9609 - val_acc: 0.0000e+00\n",
      "Epoch 167/500\n",
      "810/810 [==============================] - 0s - loss: 13356.7930 - acc: 0.0000e+00 - val_loss: 49588.9961 - val_acc: 0.0000e+00\n",
      "Epoch 168/500\n",
      "810/810 [==============================] - 0s - loss: 12818.7471 - acc: 0.0000e+00 - val_loss: 49398.8477 - val_acc: 0.0000e+00\n",
      "Epoch 169/500\n",
      "810/810 [==============================] - 2s - loss: 12726.9664 - acc: 0.0000e+00 - val_loss: 49213.3086 - val_acc: 0.0000e+00\n",
      "Epoch 170/500\n",
      "810/810 [==============================] - 2s - loss: 13068.4168 - acc: 0.0000e+00 - val_loss: 49032.0703 - val_acc: 0.0000e+00\n",
      "Epoch 171/500\n",
      "810/810 [==============================] - 1s - loss: 13023.6192 - acc: 0.0000e+00 - val_loss: 48855.3477 - val_acc: 0.0000e+00\n",
      "Epoch 172/500\n",
      "810/810 [==============================] - 1s - loss: 12471.9023 - acc: 0.0000e+00 - val_loss: 48681.2578 - val_acc: 0.0000e+00\n",
      "Epoch 173/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810/810 [==============================] - 0s - loss: 12930.0521 - acc: 0.0000e+00 - val_loss: 48507.1445 - val_acc: 0.0000e+00\n",
      "Epoch 174/500\n",
      "810/810 [==============================] - 0s - loss: 12672.7460 - acc: 0.0000e+00 - val_loss: 48333.7070 - val_acc: 0.0000e+00\n",
      "Epoch 175/500\n",
      "810/810 [==============================] - 0s - loss: 12838.4647 - acc: 0.0000e+00 - val_loss: 48165.2578 - val_acc: 0.0000e+00\n",
      "Epoch 176/500\n",
      "810/810 [==============================] - 0s - loss: 12574.9104 - acc: 0.0000e+00 - val_loss: 48002.3750 - val_acc: 0.0000e+00\n",
      "Epoch 177/500\n",
      "810/810 [==============================] - 0s - loss: 13125.0480 - acc: 0.0000e+00 - val_loss: 47843.5664 - val_acc: 0.0000e+00\n",
      "Epoch 178/500\n",
      "810/810 [==============================] - 0s - loss: 12409.8567 - acc: 0.0000e+00 - val_loss: 47687.7734 - val_acc: 0.0000e+00\n",
      "Epoch 179/500\n",
      "810/810 [==============================] - 0s - loss: 12694.3532 - acc: 0.0000e+00 - val_loss: 47535.9609 - val_acc: 0.0000e+00\n",
      "Epoch 180/500\n",
      "810/810 [==============================] - 0s - loss: 12861.5505 - acc: 0.0012 - val_loss: 47386.6367 - val_acc: 0.0000e+00\n",
      "Epoch 181/500\n",
      "810/810 [==============================] - 0s - loss: 13008.5193 - acc: 0.0000e+00 - val_loss: 47240.4180 - val_acc: 0.0000e+00\n",
      "Epoch 182/500\n",
      "810/810 [==============================] - 0s - loss: 12478.3992 - acc: 0.0000e+00 - val_loss: 47094.6211 - val_acc: 0.0000e+00\n",
      "Epoch 183/500\n",
      "810/810 [==============================] - 0s - loss: 12817.6005 - acc: 0.0000e+00 - val_loss: 46948.6523 - val_acc: 0.0000e+00\n",
      "Epoch 184/500\n",
      "810/810 [==============================] - 0s - loss: 12514.5225 - acc: 0.0000e+00 - val_loss: 46808.5039 - val_acc: 0.0000e+00\n",
      "Epoch 185/500\n",
      "810/810 [==============================] - 0s - loss: 12893.0084 - acc: 0.0000e+00 - val_loss: 46672.9609 - val_acc: 0.0000e+00\n",
      "Epoch 186/500\n",
      "810/810 [==============================] - 0s - loss: 12907.2858 - acc: 0.0000e+00 - val_loss: 46538.8672 - val_acc: 0.0000e+00\n",
      "Epoch 187/500\n",
      "810/810 [==============================] - 0s - loss: 12553.0680 - acc: 0.0000e+00 - val_loss: 46406.5664 - val_acc: 0.0000e+00\n",
      "Epoch 188/500\n",
      "810/810 [==============================] - 0s - loss: 12327.3562 - acc: 0.0000e+00 - val_loss: 46278.5312 - val_acc: 0.0000e+00\n",
      "Epoch 189/500\n",
      "810/810 [==============================] - 0s - loss: 12746.3299 - acc: 0.0000e+00 - val_loss: 46151.6172 - val_acc: 0.0000e+00\n",
      "Epoch 190/500\n",
      "810/810 [==============================] - 0s - loss: 13157.9894 - acc: 0.0000e+00 - val_loss: 46027.5312 - val_acc: 0.0000e+00\n",
      "Epoch 191/500\n",
      "810/810 [==============================] - 0s - loss: 12280.3705 - acc: 0.0000e+00 - val_loss: 45903.3359 - val_acc: 0.0000e+00\n",
      "Epoch 192/500\n",
      "810/810 [==============================] - 0s - loss: 12432.8216 - acc: 0.0000e+00 - val_loss: 45781.9805 - val_acc: 0.0000e+00\n",
      "Epoch 193/500\n",
      "810/810 [==============================] - 0s - loss: 12141.5019 - acc: 0.0000e+00 - val_loss: 45660.7344 - val_acc: 0.0000e+00\n",
      "Epoch 194/500\n",
      "810/810 [==============================] - 0s - loss: 12488.0620 - acc: 0.0000e+00 - val_loss: 45542.7383 - val_acc: 0.0000e+00\n",
      "Epoch 195/500\n",
      "810/810 [==============================] - 0s - loss: 12557.1915 - acc: 0.0000e+00 - val_loss: 45431.0781 - val_acc: 0.0000e+00\n",
      "Epoch 196/500\n",
      "810/810 [==============================] - 0s - loss: 12554.3548 - acc: 0.0000e+00 - val_loss: 45320.1836 - val_acc: 0.0000e+00\n",
      "Epoch 197/500\n",
      "810/810 [==============================] - 0s - loss: 12605.6195 - acc: 0.0000e+00 - val_loss: 45209.5977 - val_acc: 0.0000e+00\n",
      "Epoch 198/500\n",
      "810/810 [==============================] - 0s - loss: 12101.9341 - acc: 0.0000e+00 - val_loss: 45100.1602 - val_acc: 0.0000e+00\n",
      "Epoch 199/500\n",
      "810/810 [==============================] - 0s - loss: 12665.6947 - acc: 0.0000e+00 - val_loss: 44991.7578 - val_acc: 0.0000e+00\n",
      "Epoch 200/500\n",
      "810/810 [==============================] - 0s - loss: 12254.9375 - acc: 0.0000e+00 - val_loss: 44887.3750 - val_acc: 0.0000e+00\n",
      "Epoch 201/500\n",
      "810/810 [==============================] - 0s - loss: 12320.5547 - acc: 0.0012 - val_loss: 44784.0039 - val_acc: 0.0000e+00\n",
      "Epoch 202/500\n",
      "810/810 [==============================] - 0s - loss: 12312.0865 - acc: 0.0012 - val_loss: 44685.4336 - val_acc: 0.0000e+00\n",
      "Epoch 203/500\n",
      "810/810 [==============================] - 0s - loss: 12247.2711 - acc: 0.0000e+00 - val_loss: 44586.3828 - val_acc: 0.0000e+00\n",
      "Epoch 204/500\n",
      "810/810 [==============================] - 0s - loss: 12642.5464 - acc: 0.0012 - val_loss: 44489.1562 - val_acc: 0.0000e+00\n",
      "Epoch 205/500\n",
      "810/810 [==============================] - 0s - loss: 12440.2949 - acc: 0.0000e+00 - val_loss: 44395.0312 - val_acc: 0.0000e+00\n",
      "Epoch 206/500\n",
      "810/810 [==============================] - 0s - loss: 12339.4644 - acc: 0.0000e+00 - val_loss: 44302.1094 - val_acc: 0.0000e+00\n",
      "Epoch 207/500\n",
      "810/810 [==============================] - 0s - loss: 12324.2360 - acc: 0.0000e+00 - val_loss: 44209.5312 - val_acc: 0.0000e+00\n",
      "Epoch 208/500\n",
      "810/810 [==============================] - 0s - loss: 12275.3823 - acc: 0.0000e+00 - val_loss: 44115.4570 - val_acc: 0.0000e+00\n",
      "Epoch 209/500\n",
      "810/810 [==============================] - 0s - loss: 12124.2652 - acc: 0.0000e+00 - val_loss: 44019.8203 - val_acc: 0.0000e+00\n",
      "Epoch 210/500\n",
      "810/810 [==============================] - 0s - loss: 12863.6952 - acc: 0.0000e+00 - val_loss: 43926.6523 - val_acc: 0.0000e+00\n",
      "Epoch 211/500\n",
      "810/810 [==============================] - 0s - loss: 12157.7239 - acc: 0.0000e+00 - val_loss: 43838.4492 - val_acc: 0.0000e+00\n",
      "Epoch 212/500\n",
      "810/810 [==============================] - 0s - loss: 12543.9501 - acc: 0.0000e+00 - val_loss: 43748.5039 - val_acc: 0.0000e+00\n",
      "Epoch 213/500\n",
      "810/810 [==============================] - 0s - loss: 12393.9953 - acc: 0.0000e+00 - val_loss: 43659.9922 - val_acc: 0.0000e+00\n",
      "Epoch 214/500\n",
      "810/810 [==============================] - 0s - loss: 12922.9355 - acc: 0.0000e+00 - val_loss: 43572.3828 - val_acc: 0.0000e+00\n",
      "Epoch 215/500\n",
      "810/810 [==============================] - 0s - loss: 12737.2191 - acc: 0.0000e+00 - val_loss: 43488.6875 - val_acc: 0.0000e+00\n",
      "Epoch 216/500\n",
      "810/810 [==============================] - 0s - loss: 12721.8201 - acc: 0.0000e+00 - val_loss: 43409.5039 - val_acc: 0.0000e+00\n",
      "Epoch 217/500\n",
      "810/810 [==============================] - 0s - loss: 12648.2900 - acc: 0.0000e+00 - val_loss: 43329.9023 - val_acc: 0.0000e+00\n",
      "Epoch 218/500\n",
      "810/810 [==============================] - 0s - loss: 12507.5764 - acc: 0.0000e+00 - val_loss: 43252.3789 - val_acc: 0.0000e+00\n",
      "Epoch 219/500\n",
      "810/810 [==============================] - 0s - loss: 12325.1117 - acc: 0.0000e+00 - val_loss: 43177.8555 - val_acc: 0.0000e+00\n",
      "Epoch 220/500\n",
      "810/810 [==============================] - 0s - loss: 12471.1667 - acc: 0.0000e+00 - val_loss: 43104.2930 - val_acc: 0.0000e+00\n",
      "Epoch 221/500\n",
      "810/810 [==============================] - 0s - loss: 12479.3436 - acc: 0.0012 - val_loss: 43030.6719 - val_acc: 0.0000e+00\n",
      "Epoch 222/500\n",
      "810/810 [==============================] - 0s - loss: 11909.9424 - acc: 0.0000e+00 - val_loss: 42957.5977 - val_acc: 0.0000e+00\n",
      "Epoch 223/500\n",
      "810/810 [==============================] - 0s - loss: 12514.4664 - acc: 0.0000e+00 - val_loss: 42884.1289 - val_acc: 0.0000e+00\n",
      "Epoch 224/500\n",
      "810/810 [==============================] - 0s - loss: 12445.6421 - acc: 0.0000e+00 - val_loss: 42812.2266 - val_acc: 0.0000e+00\n",
      "Epoch 225/500\n",
      "810/810 [==============================] - 0s - loss: 12882.3592 - acc: 0.0000e+00 - val_loss: 42740.7695 - val_acc: 0.0000e+00\n",
      "Epoch 226/500\n",
      "810/810 [==============================] - 0s - loss: 11993.0258 - acc: 0.0000e+00 - val_loss: 42671.5742 - val_acc: 0.0000e+00\n",
      "Epoch 227/500\n",
      "810/810 [==============================] - 0s - loss: 12757.8973 - acc: 0.0000e+00 - val_loss: 42604.9961 - val_acc: 0.0000e+00\n",
      "Epoch 228/500\n",
      "810/810 [==============================] - 0s - loss: 11989.2789 - acc: 0.0000e+00 - val_loss: 42540.8008 - val_acc: 0.0000e+00\n",
      "Epoch 229/500\n",
      "810/810 [==============================] - 0s - loss: 11866.4751 - acc: 0.0000e+00 - val_loss: 42479.2031 - val_acc: 0.0000e+00\n",
      "Epoch 230/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810/810 [==============================] - 0s - loss: 11985.5334 - acc: 0.0000e+00 - val_loss: 42418.3008 - val_acc: 0.0000e+00\n",
      "Epoch 231/500\n",
      "810/810 [==============================] - 0s - loss: 11821.4493 - acc: 0.0000e+00 - val_loss: 42358.4883 - val_acc: 0.0000e+00\n",
      "Epoch 232/500\n",
      "810/810 [==============================] - 0s - loss: 12785.4979 - acc: 0.0000e+00 - val_loss: 42300.6719 - val_acc: 0.0000e+00\n",
      "Epoch 233/500\n",
      "810/810 [==============================] - 0s - loss: 12414.7511 - acc: 0.0000e+00 - val_loss: 42242.5156 - val_acc: 0.0000e+00\n",
      "Epoch 234/500\n",
      "810/810 [==============================] - 0s - loss: 12125.1002 - acc: 0.0000e+00 - val_loss: 42187.0234 - val_acc: 0.0000e+00\n",
      "Epoch 235/500\n",
      "810/810 [==============================] - 0s - loss: 12356.9433 - acc: 0.0000e+00 - val_loss: 42130.9648 - val_acc: 0.0000e+00\n",
      "Epoch 236/500\n",
      "810/810 [==============================] - 0s - loss: 12785.0042 - acc: 0.0000e+00 - val_loss: 42072.8047 - val_acc: 0.0000e+00\n",
      "Epoch 237/500\n",
      "810/810 [==============================] - 0s - loss: 12334.4978 - acc: 0.0000e+00 - val_loss: 42013.9414 - val_acc: 0.0000e+00\n",
      "Epoch 238/500\n",
      "810/810 [==============================] - 0s - loss: 12703.2535 - acc: 0.0000e+00 - val_loss: 41957.7266 - val_acc: 0.0000e+00\n",
      "Epoch 239/500\n",
      "810/810 [==============================] - 0s - loss: 12565.1488 - acc: 0.0000e+00 - val_loss: 41902.9922 - val_acc: 0.0000e+00\n",
      "Epoch 240/500\n",
      "810/810 [==============================] - 0s - loss: 12301.4428 - acc: 0.0000e+00 - val_loss: 41847.7734 - val_acc: 0.0000e+00\n",
      "Epoch 241/500\n",
      "810/810 [==============================] - 0s - loss: 12584.5815 - acc: 0.0000e+00 - val_loss: 41790.6602 - val_acc: 0.0000e+00\n",
      "Epoch 242/500\n",
      "810/810 [==============================] - 0s - loss: 12346.6260 - acc: 0.0000e+00 - val_loss: 41739.2812 - val_acc: 0.0000e+00\n",
      "Epoch 243/500\n",
      "810/810 [==============================] - 0s - loss: 12400.6810 - acc: 0.0000e+00 - val_loss: 41685.1641 - val_acc: 0.0000e+00\n",
      "Epoch 244/500\n",
      "810/810 [==============================] - 0s - loss: 12557.2064 - acc: 0.0012 - val_loss: 41634.0977 - val_acc: 0.0000e+00\n",
      "Epoch 245/500\n",
      "810/810 [==============================] - 0s - loss: 11710.1425 - acc: 0.0000e+00 - val_loss: 41584.3555 - val_acc: 0.0000e+00\n",
      "Epoch 246/500\n",
      "810/810 [==============================] - 0s - loss: 12618.0418 - acc: 0.0000e+00 - val_loss: 41535.9023 - val_acc: 0.0000e+00\n",
      "Epoch 247/500\n",
      "810/810 [==============================] - 0s - loss: 12676.4356 - acc: 0.0000e+00 - val_loss: 41489.9570 - val_acc: 0.0000e+00\n",
      "Epoch 248/500\n",
      "810/810 [==============================] - 0s - loss: 12428.1885 - acc: 0.0000e+00 - val_loss: 41441.7812 - val_acc: 0.0000e+00\n",
      "Epoch 249/500\n",
      "810/810 [==============================] - 0s - loss: 12462.7933 - acc: 0.0000e+00 - val_loss: 41388.9414 - val_acc: 0.0000e+00\n",
      "Epoch 250/500\n",
      "810/810 [==============================] - 0s - loss: 12481.1776 - acc: 0.0000e+00 - val_loss: 41334.5156 - val_acc: 0.0000e+00\n",
      "Epoch 251/500\n",
      "810/810 [==============================] - 0s - loss: 12289.5289 - acc: 0.0000e+00 - val_loss: 41281.2031 - val_acc: 0.0000e+00\n",
      "Epoch 252/500\n",
      "810/810 [==============================] - 0s - loss: 12095.7804 - acc: 0.0012 - val_loss: 41227.4922 - val_acc: 0.0000e+00\n",
      "Epoch 253/500\n",
      "810/810 [==============================] - 0s - loss: 12911.9681 - acc: 0.0000e+00 - val_loss: 41178.7070 - val_acc: 0.0000e+00\n",
      "Epoch 254/500\n",
      "810/810 [==============================] - 0s - loss: 12003.5984 - acc: 0.0000e+00 - val_loss: 41134.5273 - val_acc: 0.0000e+00\n",
      "Epoch 255/500\n",
      "810/810 [==============================] - 0s - loss: 12229.8876 - acc: 0.0000e+00 - val_loss: 41090.4492 - val_acc: 0.0000e+00\n",
      "Epoch 256/500\n",
      "810/810 [==============================] - 0s - loss: 12546.9573 - acc: 0.0000e+00 - val_loss: 41044.8008 - val_acc: 0.0000e+00\n",
      "Epoch 257/500\n",
      "810/810 [==============================] - 0s - loss: 12139.1619 - acc: 0.0000e+00 - val_loss: 41002.0000 - val_acc: 0.0000e+00\n",
      "Epoch 258/500\n",
      "810/810 [==============================] - 0s - loss: 12033.1251 - acc: 0.0000e+00 - val_loss: 40961.4414 - val_acc: 0.0000e+00\n",
      "Epoch 259/500\n",
      "810/810 [==============================] - 0s - loss: 12087.9063 - acc: 0.0000e+00 - val_loss: 40923.9297 - val_acc: 0.0000e+00\n",
      "Epoch 260/500\n",
      "810/810 [==============================] - 0s - loss: 12273.5770 - acc: 0.0012 - val_loss: 40884.0977 - val_acc: 0.0000e+00\n",
      "Epoch 261/500\n",
      "810/810 [==============================] - 0s - loss: 12370.7639 - acc: 0.0000e+00 - val_loss: 40844.1758 - val_acc: 0.0000e+00\n",
      "Epoch 262/500\n",
      "810/810 [==============================] - 0s - loss: 12492.7353 - acc: 0.0000e+00 - val_loss: 40807.0000 - val_acc: 0.0000e+00\n",
      "Epoch 263/500\n",
      "810/810 [==============================] - 0s - loss: 12486.8964 - acc: 0.0000e+00 - val_loss: 40773.8828 - val_acc: 0.0000e+00\n",
      "Epoch 264/500\n",
      "810/810 [==============================] - 0s - loss: 12345.1662 - acc: 0.0000e+00 - val_loss: 40741.1250 - val_acc: 0.0000e+00\n",
      "Epoch 265/500\n",
      "810/810 [==============================] - 0s - loss: 12105.7769 - acc: 0.0000e+00 - val_loss: 40711.0703 - val_acc: 0.0000e+00\n",
      "Epoch 266/500\n",
      "810/810 [==============================] - 0s - loss: 12283.7078 - acc: 0.0000e+00 - val_loss: 40680.4219 - val_acc: 0.0000e+00\n",
      "Epoch 267/500\n",
      "810/810 [==============================] - 0s - loss: 12073.5281 - acc: 0.0000e+00 - val_loss: 40649.1406 - val_acc: 0.0000e+00\n",
      "Epoch 268/500\n",
      "810/810 [==============================] - 0s - loss: 11754.4235 - acc: 0.0000e+00 - val_loss: 40617.5859 - val_acc: 0.0000e+00\n",
      "Epoch 269/500\n",
      "810/810 [==============================] - 0s - loss: 12279.6396 - acc: 0.0000e+00 - val_loss: 40583.1484 - val_acc: 0.0000e+00\n",
      "Epoch 270/500\n",
      "512/810 [=================>............] - ETA: 0s - loss: 12467.4971 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-015e9ebe4bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     validation_split=0.1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    epochs=500,\n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00 MSE (0.02 RMSE)\n",
      "Test Score: 0.00 MSE (0.03 RMSE)\n"
     ]
    }
   ],
   "source": [
    "trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test[-1])\n",
    "diff=[]\n",
    "ratio=[]\n",
    "p = model.predict(X_test)\n",
    "for u in range(len(y_test)):\n",
    "    pr = p[u][0]\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))\n",
    "    #print(u, y_test[u], pr, (y_test[u]/pr)-1, abs(y_test[u]- pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions vs Real results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzejuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef4dd69898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "plt2.plot(p,color='red', label='prediction')\n",
    "plt2.plot(y_test,color='blue', label='y_test')\n",
    "plt2.legend(loc='upper left')\n",
    "plt2.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
